---
title: "K Means Clustering"
author: "R. G. Cronce"
date: "July 23, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K Means Clustering

Not to be confused with K-Nearest Neighbors, this is a method of assigning data to K groups such that the spread within the groups is minimized while the distance between the centroids of the groups is maximized.  This is a built-in in R and is also presented here based on first principles.  It is not clear that the home-brew code will always converge to a fixed solution so I woud strongly recommend using the kmeans() function instead.


[R-bloggers](https://www.r-bloggers.com/learning-data-science-understanding-and-using-k-means-clustering/)

[Original Website](http://blog.ephorie.de/learning-data-science-understanding-and-using-k-means-clustering)

After fitting a synthetic example, also used to fit some *real-world* data:

[Data Set](https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale customers data.csv)

```{r}
n <- 3 # no. of centroids
set.seed(1415) # set seed for reproducibility
 
M1 <- matrix(round(runif(100, 1, 5), 1), ncol = 2)
M2 <- matrix(round(runif(100, 7, 12), 1), ncol = 2)
M3 <- matrix(round(runif(100, 20, 25), 1), ncol = 2)
M <- rbind(M1, M2, M3)
 
C <- M[1:n, ] # define centroids as first n objects
obs <- length(M) / 2
A <- sample(1:n, obs, replace = TRUE) # assign objects to centroids at random
colors <- seq(10, 200, 25) 
 
clusterplot <- function(M, A, C, txt) {
  plot(M, main = txt, xlab = "", ylab = "")
  for(i in 1:n) {
    points(C[i, , drop = FALSE], pch = 23, lwd = 3, col = colors[i])
    points(M[A == i, , drop = FALSE], col = colors[i])    
  }
}
clusterplot(M, A, C, "Initialization")
```

As you can see above, there are clearly three clusters of data but they have been arbitrarily assigned to different groups and they are not optimized.  The next set of logic assigns the data to the clusters in order to minimize/maximize the statistics noted above.

Data values are defined as follows:

    n   = number of clusters
    obs = number of data values (observations)
    M[] = data values (columns = x,y)
    C[] = centroids of the clusters, initially set to first three data points
    A[] = vector of the cluster assignments, initially set to random values
    colors[] = vector of color values = 10  35  60  85 110 135 160 185
        Displayed colors (found via a plotting test) are:
        red, green, blue, cyan, magenta, yellow, gray, black
    
    clusterplot(M,A,C,txt) = function to plot the cluster data
    
        M = data values (plotted as circles)
        A = cluster assignments (used to set the color of the points)
        C = centroid values (plotted as bold diamonds)

        Warning -- as originally written this function was dangerous and wrong
        since it accesssed A[] from calling scope and made assumptions about the
        order of the rows in C versus the cluster values in A!
        
        In order to match the color of the centroid to the data the values in C must
        be sorted into the same order as the cluster assignments in A.  While this is
        true in the home-brew code it is not true for the centers generated by kmeans().
        
        Code was modified to pass in the cluster assignments (in A).
 
 
 The code below iterates on the data and calculates the distance between each data point and
 each of the cluster centroids (1:n).  The shorted distance is used to assign the new cluster
 values.  The is done below as follows:
 
    D[i,j] = matrix of distances with one row for each centroid
    max.col(t(-D)) requires some explanation.  This assigns the data to the clusters.
    
        max.col(M) returns the index of the largest value in each row of M.
        In this case, index means the column number for each row.
        This is a base R function.
        
        max.col(t(M)) returns the index of the largest value in each column of M.
        
        max.col(t(-M)) returns the index of the smallest value in each column of M.
 
        Note the trick of using max.col(-M) to get the smallest value since there is no
        function for min.col().

So the code calculates the distances and assigns cluster values (the E-step).  After this is
done the new cluster values are compared to the old ones.  If they are the same then we are
done.  If they are not the same then the new centroids are calculated (the M-step).  After
this we start over.  All plots are generated after the stopping decision is made so you always
get the E-step and M-step plot for each time through the loop.

Note that it is not intuitively obvious that this loop has to terminate.  I could imagine it
getting stuck in a limit cycle with the allocation oscillating between two different assignments.

```{r}
repeat {
  # calculate Euclidean distance between objects and centroids
  D <- matrix(data = NA, nrow = n, ncol = obs)
  for(i in 1:n) {
    for(j in 1:obs) {
      D[i, j] <- sqrt((M[j, 1] - C[i, 1])^2 + (M[j, 2] - C[i, 2])^2)
    }
  }
  O <- A
   
  ## E-step: parameters are fixed, distributions are optimized
  A <- max.col(t(-D)) # assign objects to centroids
  if(all(O == A)) break # if no change stop
  clusterplot(M, A, C, "E-step")
   
  ## M-step: distributions are fixed, parameters are optimized
  # determine new centroids based on mean of assigned objects
  for(i in 1:n) {
    C[i, ] <- apply(M[A == i, , drop = FALSE], 2, mean)
  }
  clusterplot(M, A, C, "M-step")
}
```

The centroids are initially (arbitrarily) assigned to the first three data points.  After that, the centroids and cluster assignments are modified by the E and M steps as shown above.

These results eventually become stable and can be compared to the built-in function as shown below:

```{r}
cl <- kmeans(M, n)
clusterplot(M, cl$cluster, cl$centers, "Base R")
```

Note that using the original code the color of the centroids did not match the color of the points!  Why?  The colors matched on the website.

I checked and found that the cluster assignments in the home-brew code `(C,A)` aren't in the same
order as the kmeans function `(cl$centers, cl$cluster)`.  I modified the plotting function to
pass in the cluster vector and at least the centers and cluster data agree.  The two plots
are still different since there is no particular definition of what the three different
cluster values should be.

The home-brew code assigns cluster values in the order of 2, 1, 3 whereas the kmeans() code
assigns 1, 3, 2.  These get mapped to 1=red, 2=green, 3=blue by the plotting routine.


```{r}
print("Custom K-Means Code")
(custom <- C[order(C[ , 1]), ])
print("Using R built-in Code")
(base <- cl$centers[order(cl$centers[ , 1]), ])
print("Difference")
round(base - custom, 13)
```

Comparison of the home-brew code versus the built-in kmeans function shows they yield the same result.
The cluster centers are put into order of increasing x-value for the purposes of the comparison.



## Real-World Example

I'm not sure I have a lot to add to this discussion.  I have a lot of trouble visualizing a six dimensional structure clustered into 4 groups.  It would be a lot better to have an example that could be visualized in two or three dimensions (with color representing clusters).

I ran the problem in the console using just the Fresh and Frozen columns.  The within and between ss indicates there is a lot of spread in the data and the clusters are not very distinct.  About 77% of the variation is between clusters but, of course, that means there is 23% within.  Using the full set of data is even worse with 57% between and 43% within.  This is much messier than the artificial example at the start of this investigation where the between variation was 97.4% and only 2.6% within the clusters.  I would guess that for real-world problems with a large number of variables/clusters this is very difficult to intepret.


```{r}
t <- "Wholesale customers data.csv"
if (file.exists(t)) {
  data <- read.csv(t, header=TRUE)
} else {
  data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale customers data.csv", header = TRUE)
}
head(data)
```

The fitting process is shown below.  Note that the random number seed is needed to make the results deterministic.  There is apparently some random sampling that goes on at the start of the fitting process that affects the cluster results.


```{r}
set.seed(123)
k <- kmeans(data[ , -c(1, 2)], centers = 4) # remove columns 1 and 2, create 4 clusters
(centers <- k$centers) # display cluster centers
```

```{r}
round(prop.table(centers, 2) * 100) # percentage of sales per category
```

```{r}
table(k$cluster) # number of customers per cluster
```

Per the original article:

One interpretation could be the following for the four clusters:

* Big general shops
* Small food shops
* Big food shops
* Small general shops

As you can see, the interpretation of some clusters found by the algorithm can be quite a challenge. If you have a better idea of how to interpret the result please tell me in the comments below!

